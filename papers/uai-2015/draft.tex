\documentclass[]{article}
\usepackage{proceed2e}

% Set the typeface to Times Roman
\usepackage{times}


% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subcaption} 
%usepackage[numers]{natbib}
% For citations
%\usepackage[numbers]{natbib}

% For algorithms
%\usepackage{algorithm}
%\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
%\usepackage{hyperref}
% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{bm}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
%\usepackage{array}
%\usepackage{nips_style} 
%\usepackage{sidecap}
%\usepackage[export]{adjustbox}
%\include{cdefs}

% Leave date blank
\date{}

\pagestyle{myheadings}
%\usepackage{sidecap}
%\usepackage[export]{adjustbox}

%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\input{cdefs}


\title{Hamiltonian ABC}

\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.
          % UAI  reviewing is double-blind.
          
% \author{
% Edward Meeds \\
% Informatics Institute \\
% University of Amsterdam \\
% \texttt{tmeeds@gmail.com} \\
% \and
% \author{
% Robert Leenders \\
% Informatics Institute \\
% University of Amsterdam \\
% \texttt{robert.leenders@student.uva.nl} \\
% \and
% Max Welling \\
% Informatics Institute\\
% University of Amsterdam \\
%  \texttt{welling.max@gmail.com}
% }
%\nipsfinalcopy
\begin{document} 
	\vskip -0.3in
  
\maketitle

% 
% 
% \begin{abstract} 
% \input{abstract}
% \end{abstract} 
\begin{abstract} 
  Approximate Bayesian computation (ABC) is a powerful and elegant framework for performing inference in simulation-based models.  However, due to the difficulty in scaling likelihood estimates, ABC remains useful for relatively low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of likelihood-free algorithms that apply recent advances in scaling Bayesian learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients.     We find that a small number forward simulations can effectively approximate the ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces.  We also describe a new simple yet general approach of incorporating random seeds into the state of the Markov chain, further reducing the random walk behavior of HABC.  We demonstrate HABC on several typical ABC problems, and show that HABC performs comparably to regular Bayesian inference on a high-dimensional problem from machine learning.
  %
  %
  % Models in simulation-based science domains are considered likelihood-free (LF) since they are not defined by a probabilistic model, but by a simulator that maps parameters to pseudo-observations.  To perform parameter inference in the models, a powerful set of statistical procedures called Approximate Bayesian Computation (ABC) can be applied.  Although there is a wide variety of ABC algorithms, they are dominated by SMC/PMC and MCMC.  These have been successful, but their inefficiency of producing samples from the approximate posterior has limited ABC to simulations with relatively few parameters.  Hamiltonian Monte Carlo (HMC)  provides a framework for extending ABC to high-dimensions, but requires computing the gradient of the log-likelihood.  With Hamiltonian ABC, we show how recent advances of stochastic gradient HMC algorithms applied to the big data domain can be also used for efficient sampling for LF models.  We find that a small number forward simulations can effectively approximate the true ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces.   We demonstrate HABC on several typical ABC problems, and show that HABC performs comparably to regular Bayesian inference on a high-dimensional problem from machine learning.
%
%
%   Bayesian inference in simulation-based science relies
%   Approximate Bayesian Computation (ABC) is a set of inference procedures
%   for so-called {\em likelihood-free} models
%
% In this paper we apply recent techniques from Bayesian inference that use gradient information to sample efficiently from the true posterior distribution to the {\em likelihood-free} or {\em approximate Bayesian computation} (ABC) setting.  To do this for ABC we adopt a {\em gradient-free} stochastic approximation algorithm by Spall~\cite{spall1999}.  Together these algorithms provide both optimization and inference for likelihood-free models as the algorithm ABC-SGLD transitions from optimization to sampling.  We demonstrate ABC-SGLD on problems where the true gradient information is known and on challenging ABC simulators.
\end{abstract} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION} \label{introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In simulation-based science, models are defined by a simulator and its parameters.  These are called {\em likelihood-free} models because, in contrast to probabilistic models, their likelihoods are either intractable to compute or must be approximated by simulations.  To perform inference in likelihood-free models, a broad class of algorithms called Approximate Bayesian Computation \cite{beaumont2002approximate,marjoram2003markov,sisson2007sequential,sisson:2010,marin:2012,fan:2013} are employed.

At the core of every ABC algorithm is simulation.  To evaluate the quality of a parameter vector $\thetav$, a simulation is run using $\thetav$ as inputs and producing outputs $\x$.  If the pseudo-data $\x$ is ``close'' to observations $\y$, then $\thetav$ is kept as a sample from the approximate posterior.  Parameters $thetav$ are then adjusted, depending upon the algorithm, to obtain the next sample.

In ABC, there is a fundamental trade-off between the computation required to obtain independent samples and the approximation to the true posterior.  If the parameter measuring closeness is too small, then samplers ``mix'' poorly; on the other hand, if it is too large, then the approximation is poor.  As the dimension of the parameters grows, the problem worsens, just as it does for general Bayesian inference with probabilistic models, but it is more acute for ABC due to its simulation requirement.  There is therefore a deep interest in improving the efficiency of ABC samplers (in terms of computation per independent sample).  In this paper we address this issue directly by using Hamiltonian dynamics to approximately sample from likelihood-free models with high-dimensional parameters.

Hamiltonian Monte Carlo (HMC) \cite{duane1987hybrid, neal2011mcmc} is perhaps the only Bayesian inference algorithm that scales to high-dimensional parameter spaces.  The core computation of HMC is the gradient of the log-likelihood.  Two problems arise if we consider HMC for ABC: one, how can the gradients be computed for high-dimensional likelihood-free models, and two, given a stochastic approximation to the gradient, can a valid HMC algorithm be derived?
 
 To answer the latter, we turn to recent developments in scaling Bayesian inference using HMC and stochastic gradients \cite{welling2011bayesian,chen2014stochastic,ding2014bayesian}.  We call these {\em stochastic gradient Hamiltonian dynamics} (SGHD) algorithms. SGHD are computationally efficient for two reasons.  First, they avoid computing the gradient of the log-likelihood over the entire data set, instead approximating it using small batches of data, i.e. computing stochastic gradients.  Second, they can maintain reasonable approximations to the Hamiltonian dynamics and therefore avoid a Metropolis-Hastings correction step involving the full data set.  Different strategies are employed to do this: small step-sizes combined with  Langevin dynamics \cite{welling2011bayesian}, using friction to prevent accumulation of errors in the Hamiltonian \cite{chen2014stochastic}, and using a thermostat to control the temperature of the Hamiltonian \cite{ding2014bayesian}.  Each of these strategies can be used by HABC.
 
%Recent advances in Bayesian inference within the {\em big data} domain have combined the efficiency of stochastic gradient algorithms used for optimization with Hamiltonian dynamics;  these include Stochastic Gradient Langevin Dynamics (SGLD) \cite{welling2011bayesian}, Stochastic Gradient HMC \cite{chen2014stochastic}, and Stochastic Gradient Thermostats \cite{ding2014bayesian}.
%By using small batches of the full data set these methods are able to quickly compute a stochastic approximation to the true gradient, which with a small enough step-size and with an appropriate amount of injected noise, can produce samples from the true posterior.  The Hamiltonian dynamics avoid random walk behavior by providing momentum to the sampler allowing it to reach far away regions of parameter space.  Critically for the big data domain, the small step-sizes prevent the trajectories of the sampler from accumulating too much error and therefore they can avoid a Metropolis-Hastings correction step involving the full data set.

In HABC, we use forward simulations to approximate the likelihood-free gradient. The key difference between SGHD methods and HABC is that the stochasticity of the gradient does not come from approximating the full data gradient with a batch gradient, but by the stochasticity of the simulator.  It is therefore not the expense of the simulator (though this could very well be the case for many interesting simulation-based models -- see Section~\ref{sec:conclusion}) that requires an approximation to the gradient, but the likelihood-free nature of the problem.  

There are several difficulties in estimating gradients of likelihood-free models that we address with HABC.  The first is do to the form of the ABC log-likelihood.  As we show in Section~\ref{sec:abc}, using a conditional model for $\pi( \x | \thetav )$ provides an estimate of the ABC likelihood that is less sensitive to $\eps$ and therefore is more conducive to stochastic gradient computations.  The second difficulty is that for high-dimensional parameter spaces, computing the gradients naively (i.e. by finite differences \cite{kiefer1952stochastic}) can squash the gains brought by the Hamiltonian dynamics.  Fortunately, we can use existing stochastic approximation algorithms \cite{spall1992multivariate,spall2000adaptive} that can be used to compute unbiased estimators of the gradient with a small number of forward simulations that is {\em independent} of the parameter dimension.  The {\em stochastic perturbation stochastic approximation} (SPSA) \cite{spall1992multivariate} is described in Section~\ref{sec:habc}




%Approximate Bayesian Computation (ABC) is a set of inference procedures for so-called {\em likelihood-free} models \cite{abccitations}.  In contrast to typical Bayesian modeling, where the parameters of a probabilistic model of data are the focus, the primary interest in ABC are the parameters of a simulation model.  The simulator can generate pseudo or auxiliary data by running the simulator with a set of parameter values, but it cannot compute a likelihood directly.  Inference involves running simulations, comparing pseudo data with observations, and using a distance between the two as an approximate likelihood.  Markov chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) are the most common inference algorithms.  It can be shown that if the statistics used in the likelihood function are sufficient, then these algorithms sample correctly from an approximation to the true posterior.

%The approximation to the true posterior can be made arbitrarily small, in principle, by controlling the kernel width parameter $\epsilon$.  As $\epsilon \rightarrow 0$, however, simulated data must be closer and closer to the observations (or receive zero likelihood).  The acceptance rate of a Markov chain for small $\epsilon$ can therefore be low.  This inefficiency of ABC sampling is exacerbated by the random walk proposals.  As the dimensionality of the parameter space grows,  $\epsilon$ must grow to prevent low acceptance rates.  This has perhaps prevented application of ABC to high-dimensional simulators.  Our paper addresses this directly: can we use the sampling efficiency of Hamiltonian dynamics (i.e. high acceptance rates) in high-dimensional simulators? 

%In this paper we adopt the developments in scaling Bayesian inference to ABC.  To do this we use forward simulations to approximate the likelihood-free gradient.  The key difference with SG methods mentioned above is that the stochasticity of the gradient does not come from approximating the full data gradient with a batch gradient, but by the stochasticity of the simulator.  It is therefore not the expense of the simulator (though this could very well be the case for many interesting simulation-based models, and we address this in Future Work) that requires an approximation to the gradient, but the likelihood-free nature of the problem.  For high-dimensional parameter spaces, computing the gradients naively (i.e. by finite differences) can squash the gains brought by the Hamiltonian dynamics; to address this further approximations to the gradient can be used \cite{spall}.

%The approximate gradients used by HABC can be noisy and difficult to manage is care is not taken.  To address this we show how a local Gaussian approximation of the simulator output can improve the stability of the gradient estimates over a direct estimate using the ABC kernel function.  

A further innovation of this paper is the use of common random numbers (CRN) to improve the efficiency of the Hamiltonian dynamics.  The idea behind CRNs is to use the same set of random seeds for estimating a gradient by FD or SPSA, i.e. when simulating $\pi(\x| \theta + d\theta)$ and $\pi(\x| \theta - d\theta)$ use the same random seeds.  This was applied successfully to SPSA \cite{kleinman1999simulation} (and is analogous to using the same mini-batch in stochastic gradient methods).  We extend and simplify this approach by including the random seeds $\omega$ into the state of the Markov chain;  by keeping the random seeds fixed for several consecutive steps, the second order gradient stochasticity is greatly reduced.  We show that this doing this produces is a valid MCMC procedure.  This approach is not exclusive to HABC; we apply successfully to ABC-MCMC as well.  
 
We briefly review ABC in Section~\ref{sec:abc}.  In Section~\ref{sec:scaling} we review three approaches to stochastic gradient inference using Hamiltonian dynamics: SGLD, SGHMC, and SGNHT.  We then introduce Hamiltonian ABC in Section~\ref{sec:habc}, where  we will show how to improve the stability of the gradient estimates by using CRNs and local density estimators of the simulator.  Extensions to high-dimensional parameter spaces are also discussed.  In Section~\ref{sec:demo} we show how HABC behaves on a simple one-dimensional problem, then in Section~\ref{sec:experiments} we compare HABC with ABC-MCMC for two problems: a low-dimensional model of chaotic population dynamics and a high-dimensional problem. 
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{APPROXIMATE BAYESIAN COMPUTATION}\label{sec:abc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the Bayesian inference task of either drawing samples from or learning an approximate model of the following (usually intractable) posterior distribution:
\begin{equation}
  \pi(\thetav | \y_1, \ldots, \y_N ) \propto \pi(\thetav) \pi( \y_1, \ldots, \y_N  | \thetav )
\end{equation}
where $\pi(\thetav)$ is a prior distribution over parameters $\thetav \in {\rm I\!R}^{D}$ and $\pi( \y_1, \ldots, \y_N  | \thetav )$ is the likelihood of $N$ data observations, where $\y_i \in {\rm I\!R}^J$.  In ABC, the vector of $J$ observations are typically informative statistics of the raw observations.  It can be shown that if the statistics used in the likelihood function are sufficient, then these algorithms sample correctly from an approximation to the true posterior ([KEEP?]).  
  The simulator is treated as generator of random pseudo-observations, i.e. $\x \simsim \pi( \x | \thetav )$ is a draw from the simulator.  Discrepancies between the simulator outputs $\x$ and the observations $\y$ are scaled by a closeness parameter $\eps$ and treated as likelihoods.  This is the equivalent to putting an $\eps$-kernel around the observations, and using a Monte Carlo estimate of the likelihood using $S$ draws of $\x$: 
\begin{equation}
  \pi_{\epsvec}( \y | \thetav ) =  \int \pi_{\epsvec}(\y | \x ) \pi( \x | \thetav ) d\x 
                           \approx  \frac{1}{S} \sum_{s=1}^S \pi_{\epsvec}(\y | \x^{(s)} ) \label{eq:abc_mc_approx}
\end{equation}

In ABC Markov chain Monte Carlo (MCMC) \cite{marjoram2003markov,Wilkinson2013,sisson:2010} the Metropolis-Hastings (MH) proposal distribution is composed of the product of the proposal for the parameters $\thetav$ and the proposal for the simulator outputs:
\begin{equation}
  q( \thetavp, \x^{(1)'}, \ldots, \x^{(S)'} | \thetav ) =  q( \thetavp | \thetav ) \prod_s \pi( \x^{(s)'} | \thetavp)
\end{equation}
Using this form of the proposal distribution, and using the Monte Carlo approximation eq~\ref{eq:abc_mc_approx}, we arrive at the following Metropolis-Hastings accept-reject probability,
%
\begin{equation}
\alpha = \min \lp 1, \frac{\pi\lp\thetavp\rp \sum_{s=1}^S \pi_{\epsvec}(\y | \xps )  q( \thetav | \thetavp )}{\pi\lp\thetav\rp \sum_{s=1}^S \pi_{\epsvec}(\y | \xs ) q( \thetavp | \thetav )} \rp \label{eq:abc_mh_acceptance_with_s}
\end{equation}
%
If the simulations are part of the Markov chain, the algorithm corresponds to the pseudo-marginal (PM) sampler \cite{andrieu2009pseudo}, otherwise it is a marginal sampler \cite{marjoram2003markov,sisson:2010}.   For this paper we will be interested in the PM sampler because this is equivalent to having the random states that generated the simulation outputs in the state of the Markov chain, which we will use within a valid ABC sampling algorithm in Section~\ref{sec:habc}.
%
%   If they are discarded, the
% Two subtly different versions of ABC-MCMC are the pseudo-marginal (PM) \cite{delmoral2008,andrieu2009pseudo} and marginal sampler \cite{marjoram2003markov,sisson:2010}.  In PM, the simulations are part of the state of the Markov chain whereas in the marginal sampler they are not
% When only the numerator is re-estimated at every iteration (and the denominator is carried over from the previous iteration), then this algorithm corresponds to pseudo-marginal (PM) sampling \cite{delmoral2008,andrieu2009pseudo}. PM sampling is asymptotically correct (taking for granted the approximation introduced by the kernel $\pi_{\epsvec}$) but can display very poor mixing properties. By resampling the denominator as well, we improve mixing at the cost of introducing a further approximation. This sampler is known as the marginal sampler \cite{marjoram2003markov,sisson:2010}.
% [TODO: END TAKEN FROM POPE]

An alternative approach to computing the ABC likelihood is to estimate the parameters of a conditional model  $\pi(\x|\thetav)$, e.g. kernel density estimate \cite{TurnerGenLik2014} or a Gaussian model \cite{wood2010statistical}.  While either approach should be adequate and both have their own limits and advantages, for this paper we will use a Gaussian model.  In ABC, using a conditional Gaussian model for  $\pi(\x|\thetav)$ is called a {\em synthetic likelihood} (SL) model \cite{wood2010statistical}.  For a SL log-likelihood model, we compute estimators of the first and second moments of $\pi(\x|\thetav)$.  The advantage is that for a Gaussian $\epsilon$-kernel, we can convolve the two densities   
\begin{eqnarray}
  \pi_{\epsvec}( \y | \thetav ) & = & \int \mathcal{N}( \y | \x, \eps^2 ) \mathcal{N}( \x | 
  \mu_{\thetav}, \sigma^2_{\thetav} ) d\x \\
                          & = & \mathcal{N}( \y | \mu_{\thetav}, \sigma^2_{\thetav} + \eps^2 )
\end{eqnarray}

Of particular concern to this paper is the behavior of the log-likelihoods for different values of $\epsilon$.  In the $\eps$-kernel case, the log-likelihood is very sensitive to small values of $\eps$:
\begin{eqnarray}
  \log \pi_{\epsvec}( \y | \thetav ) & = & \log \sum_s \mathcal{N}( \y | \x^{(s)}, \eps^2 ) \\
                          & = & \log \mathcal{N}( \y | \x^{(s)}, \eps^2 ) + \log\lp 1 + H \rp \\
                          & \approx & -\log \eps -\frac{1}{2 \eps^2} ( \y - \x^{(m)} )^2 
\end{eqnarray}
where $m$ is the simulation that is closest to $\y$, $H$ is a sum over terms close to $0$. We can see that the log-likelihood can be set arbitrarily small by decreasing $\eps$.  On the other hand, by using a model of the simulation at $\thetav$
\begin{eqnarray}
 \log \pi_{\epsvec}( \y | \thetav ) & \approx & -\frac{1}{2} \log (\sigma^2_{\thetav} + \eps^2 ) -\frac{( \y - \mu_{\thetav} )^2 }{2 (\sigma^2_{\thetav} + \eps^2 )} 
\end{eqnarray}
 For the SL model, $\eps$ acts as a smoothing term and can be set to small values with little change to the log-likelihood, as long as the SL estimators are fit appropriately.  This insensitivity to $\eps$ will be used in Section~\ref{sec:habc} for estimating gradients of the ABC likelihood.  Before describing HABC in full detail however, we now explain how scaling Hamiltonian dynamics in Bayesian learning can be accomplished using stochastic gradients from batched data.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SCALING BAYESIAN INFERENCE USING HAMILTONIAN DYNAMICS} \label{sec:scaling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Scaling Bayesian inference algorithms to massive datasets is necessary for their existing relevance in the so-called {\em big data} era.  We now review the role stochastic gradient methods combined with Hamiltonian dynamics have played in recent advances in scaling Bayesian inference.   Most importantly, these methods have combined the ability of HMC to explore high-dimensional parameter spaces with the computational efficiency of using stochastic gradients based on small batches of the full dataset.  After an overview of HMC, we will briefly describe stochastic gradient Hamiltonian dynamics (SGHDs), starting with using  Langevin dynamics \cite{welling2011bayesian}, then HMC with friction \cite{chen2014stochastic}, and finally HMC with thermostats \cite{ding2014bayesian}.  We will then make the connection between SGHDs and HABC in Section~\ref{sec:habc}.

\subsection{Hamiltonian Monte Carlo}\label{sec:hmc}

Hamiltonian dynamics are often necessary to adequately explore the target distribution of high-dimensional parameter spaces.  By proposing parameters that are far from the current location and yet have high acceptance probability, Hamiltonian Monte Carlo \cite{duane1987hybrid, neal2011mcmc}  can efficiently avoid random walk behavior that can render proposals in high-dimensions painfully slow to mix.

HMC simulates the trajectory of a particle along a frictionless surface, using random initial momentum $\rhov$ and position $\thetav$.  The Hamiltonian function computes the energy of the system and the dynamics govern how the momentum and position change over time.  The continuous Hamiltonian dynamics can be simulated by discretizing time into small steps $\eta$.  If $\eta$ is small, the value of $\thetav$ at the end of a simulation can be used as proposals within the Metropolis-Hastings algorithm.  Hamiltonian dynamics should propose $\thetav$ that are always accepted, but errors due to discretization may require a  Metropolis-Hastings correction.  It is this correction step that SGHD algorithms want to avoid as it requires computing the log-likelihood over the full data set.

More formally, the Hamiltonian $H\lp\thetav, \rhov \rp = U(\thetav) + K(\rhov)$ is a function of the current potential energy $U(\thetav)$ and kinetic energy $K(\rhov) = \rhov^T M^{-1}\rhov/2$ ($M$ is a diagonal matrix of masses which for presentation are set to $1$).  The potential energy is defined by negative log joint density of the data and prior:
\begin{equation}
  U(\thetav) = - \log \pi(\thetav) - \sum_{i=1}^N \log \pi(\yi | \thetav )
\end{equation}
The Hamiltonian dynamics follow 
\begin{equation}
  d\thetav = \rhov dt ~~~~~~~~~~~~ d\rhov = -\nabla U(\thetav) dt
\end{equation}
in simulation $dt = \eta$. 
% Actual simulation of the Hamiltonian requires taking discrete steps, and therefore introduces some error.  The error can be corrected using a MH step, as shown in Algorithm 1 (showing leap-frog algorithm).  By only taking a single leap-frog step, HMC reduces to Langevin dynamics which can be represented as a gradient descent algorithm with injected noise.  By using small step-sizes the acceptance rate approaches 1, so that no MH correction steps are necessary and the algorithm can be run to generate a stream of (possibly highly correlated) samples from the posterior.


% Two problems must be addressed.  The first is loglikelihood computation.  For correctness, Bayesian inference algorithms require frequent computation of the (deterministic) loglikelihood over the entire observation set.  Contrast this to optimization, where stochastic gradient techniques are necessary for learning parameters.  The second problem is that massive parameter sets is synonymous with massive datasets.  As datasets grow, so too do our models.  This is particularly true in the area of deep learning, where parameter vectors start in the hundred thousands and for some models are in the billions. Hamiltonian Monte Carlo addresses this for Bayesian inference: by using gradients of the log-likelihood (log-prior) it is possible to avoid random walk behavior and more efficiently explore the posterior.

% Recent advances in scaling Bayesian inference to big data has shown that both batches and gradients can be used to correctly sample from the posterior distribution.  These use Hamiltonian dynamics based on stochastic gradients (where the stochasticity comes from approximating the true gradient with one based on a batch of data) with appropriate step-sizes (and variations, see below).  We now describe these algorithms and then show how we can adopt them for ABC.

\subsection{Stochastic Gradient Hamiltonian Dynamics}\label{se:sghd}
If the log-likelihood over the full data set is replaced with a batch estimate, as is done for the following {\em stochastic gradient Hamiltonian dynamics} (SGHDs) algorithms, then the error in simulating the Hamiltonian dynamics comes not only from the discretization, but from the variance of the stochastic gradient.  As long as this error is controlled, either by using small steps $\eta$ (SGLD), or adding friction terms $A$ (SGHMC), or using a thermostat $\xi$ (SGNHT), the expensive MH correction step can be avoided and values of $\thetav$ from the Hamiltonian dynamics can be used as samples from the posterior.

SGHDs replace the full potential energy and its gradient with a batch approximation:
\begin{eqnarray}
  \hat{U}(\thetav)       & = & - \log \pi(\thetav) - \frac{N}{n} \sum_{i=h_1}^{h_n} \log \pi(\yi | \thetav ) \\
  \nabla\hat{U}(\thetav) & = & - \nabla \log \pi(\thetav) - \frac{N}{n} \sum_{i=h_1}^{h_n} \nabla \log \pi(\yi | \thetav ) 
\end{eqnarray} 
where $n$ is the batch size, and $h_i$ are indices chosen randomly without replacement from $[1,N]$ (i.e. it defined a random batch).  
%The main motivation for these is to avoid the expensive MH correction step while maximizing the benefit of Hamiltonian dynamics.  For example, we want to take large steps but without introducing errors that cause the dynamics to fail.
 
{\bf Stochastic gradient Langevin dynamics} \cite{welling2011bayesian} performs one full leap-frog step of HMC.   Starting with a half step for the momentum, the update for $\thetav$ is 
\begin{eqnarray}
  \rhov_t & \sim & \mathcal{N}(0,\eye_p) \\
  \rhov_{t+\frac{1}{2}} & = & \rhov_t - \eta \nabla \hat{U}(\thetav_t) /2 \\
  \thetav_{t+1} & = & \thetav_t + \eta \rhov_{t+\frac{1}{2}}
\end{eqnarray}
Tt is not necessary to include $\rhov$ in the updates since there is only one step:
\begin{eqnarray}
  \thetav_{t+1} & = & \thetav_t + \eta \mathcal{N}(0,\eye_p) - \eta^2 \nabla \hat{U}(\thetav_t) /2 
\end{eqnarray}
One of the potential drawbacks of SGLD is that the momentum term is {\em refreshed} for every update of the parameters, and since this means the parameter update only uses the current gradient approximation, it limits the benefits of using Hamiltonian dynamics.  On the other hand, this also prevents SGLD from accumulating errors in the Hamiltonian dynamics.

{\bf Stochastic Gradient HMC} (SGHMC) \cite{chen2014stochastic} avoids $\rhov$ refreshment altogether.  By applying HMC directly using the stochastic approximation $\hat{U}$ and $\nabla \hat{U}$, which the authors call {\em naive SGHMC}, the variance of the gradient will introduce errors that left unaddressed will result in sampling from the incorrect target distribution.  Under the assumption that $\nabla \hat{U}(\thetav) = \nabla U(\thetav) + \mathcal{N}\lp \zerov, \varv_{\thetav}\rp$, where $\varv_{\thetav}$ is the covariance of the gradient approximation, and  updates $\rhov_{t+1} = \rhov_{t} + \Delta \rhov_{t}$ and $\thetav_{t+1} = \thetav_t + \eta \rhov_{t+1}$, the change in momenta $\Delta \rhov$ from one full step is
 \begin{equation}
   - \eta \lp \nabla U(\thetav) + \mathcal{N}\lp \zerov, \varv_{\thetav}\rp\rp  =  - \eta \nabla U(\thetav) + \mathcal{N}\lp \zerov, \eta^2 \varv_{\thetav}\rp  
 \end{equation}
By adding a friction term $\Bv$ to $\Delta \rhov$ proportional to $\varv_{\thetav}$, the correction step can be avoided
 \begin{eqnarray}
   \Delta \rhov & = & - \eta \Bv \rhov_t - \eta \nabla U(\thetav_t) + \mathcal{N}\lp \zerov, 2 \eta \Bv\rp 
 \end{eqnarray}
 where  $\Bv=\frac{1}{2}\eta \varv_{\thetav_t}$.   In practice, since we can only estimate $\Bv$ by some $\hat{\Bv}$ and can only compute $\hat{U}$, a user defined friction term $\Cv$ is used (with $\Cv - \hat{\Bv}$ is semi-positive definite).  Thus the updates used for $\Delta \rhov$ for SGHMC:
 \begin{equation}
   - \eta \Cv \rhov_t - \eta \nabla \hat{U}(\thetav_t) + \mathcal{N}\lp \zerov, 2 \eta (\Cv-\hat{\Bv}) \rp 
 \end{equation}
 In our experiments we compute an online estimate $\hat{\varv}$ and set $\Cv = c \eye_p + \hat{\varv}$.  
 %As mentioned above, the main benefit of using SGHMC over SGLD is the ability of avoiding refreshing $\rhov$ while not accumulating Hamiltonian errors.
 
 {\bf Stochastic Gradient thermostats} (SGT) \cite{ding2014bayesian} addresses the difficulty of estimating $\hat{\Bv}$ by introducing a scalar variable $\xi$ who's addition to the Hamiltonian dynamics maintains the temperature of the system constant, i.e. it acts as a (Nose-Hoovier) thermostat \cite{leimkuhler:2009}.  The update equations remain simple: initialize $\xi = \Cv$ (or $c$), then for $t=1\ldots$
 \begin{eqnarray}
   \rhov_{t+1} & = & \rho_t - \eta \xi_t \rhov - \eta \nabla \hat{U}(\thetav_t) + \mathcal{N}\lp \zerov, 2 \eta_t \Cv \rp \\
   \thetav_{t+1} & = & \thetav_t + \eta \rhov_{t+1} \\
   \xi_{t+1} & = & \xi_t + \eta \lp  \rhov_{t+1}^T \rhov_{t+1} / p - 1 \rp
 \end{eqnarray}
 In summary, the hyperparameters required for these algorithms are $\eta$ and $\Cv$ (for SGHMC and SGNHT only), and in practice, some way of estimating $\hat{\varv}$ for SGHMC.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{HAMILTONIAN ABC}\label{sec:habc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the application of stochastic gradient Hamiltonian dynamics to ABC, the stochasticity of the gradient due to batch sizes is instead caused by two other factors.  The first factor, is due to the finite number of forward simulations that are used to approximate the log-likelihood function.  We will explain below how to perform ABC gradient estimates based on finite difference approximations on two log-likelihood formulations: the $\epsilon$-kernel density model and on a Gaussian conditional model, both are based on $S$ random simulations.  The second factor is due to the dimension of $D$.  For large $D$, a finite difference estimate requires $2p$ forward simulations for each random simulation, by using a further stochastic approximation to the gradient, called Simultaneous Perturbation Stochastic Approximation (SPSA) \cite{spall}, an unbiased estimate of the gradient can computed with $2q$ forward simulations, where $ \geq 1$ and usually $Q \ll D$.

To begin, we define $\thetavplusd = \thetav + c \Deltavd$ and $\thetavminusd = \thetav + c \Deltavd$, where $c$ is a small positive scalar, e.g. $c=0.001$, and $\Deltavd$ is a vector of size $D$ that, for FD estimates is $1$ for entry $d$ and $0$ otherwise.  Our gradient estimate require simulating $S$ times at $\thetavplus$ and $\thetavminus$, generating sets of pseudo-data $\{ \xplussd \}$ and $\{ \xminussd \}$.  These are then plugged into the log-likelihood estimate for $\y$, which we call $\mathcal{L}$:
\begin{eqnarray}
  \frac{\partial \mathcal{L}(\thetav)}{\partial \thetavd} & \approx & \frac{\mathcal{L}(\thetavplusd)- \mathcal{L}(\thetavminusd)}{2c}
\end{eqnarray}

For the $\epsilon$-kernel density model
% ,
% \begin{eqnarray}
%   \mathcal{L}(\thetavpmd) & = & \log \sum_{s=1}^2 \mathcal{N}( \y | f( \thetavpmd, \omega_s ), \eps^2 ) \\
%                           & = & \log \mathcal{N}( \y | f( \thetavpmd, \omega_{m} ), \eps^2 ) + \log\lp 1 + H_d \rp \\
%                           & \approx & \log \mathcal{N}( \y | f( \thetavpmd, \omega_{m} ), \eps^2 ) + 0
% \end{eqnarray}
% where we replaced the random variable $\x$ with a deterministic function $f( \thetavpmd, \omega_s )$ (i.e.  the simulator run at $\thetavpmd$ using the random state $\omega_s$, such that $\xpmd = f( \thetavpmd, \omega_s )$). In the second line we have pulled out the value from the random state $m$ which has the maximum log-likelihood (i.e. is closest to $\y$) and used $H_d$ (sum of terms $<0$ and $\ll 0$ for small $\eps$), and in the last line simplified further by assuming small $\eps$, so that the log-likelihood is dominated by a single simulation closest to $\y$.
it is easy to show that the gradient estimate is sensitive to $\epsilon$ in this case: 
\begin{eqnarray}
  \frac{\partial \mathcal{L}(\thetav)}{\partial \thetavd} & \approx & \frac{1}{2c\eps^2}\lp -\frac{1}{2}(\y-\xplussd)^2 + \frac{1}{2}(\y-\xminussd)^2  \rp
\end{eqnarray}
So by making $\epsilon$ arbitrarily small we can make the gradient extremely sensitive to our simulations.  Intuitively, we want the gradient to be fairly stable no matter what values of $\epsilon$ we use.  To do this we will replace log-likelihood with a conditional model.
%
% A better way of approximating the gradient is to use a conditional model of $\pi(\x|\thetav)$, for example a conditional kernel density estimate \cite{TurnerGenLik2014} or a conditional Gaussian model \cite{wood2010statistical}.  While either approach should be adequate and both have their own limits and advantages, for this discussion we will use a Gaussian estimate.  In ABC, using a conditional Gaussian model for  $\pi(\x|\thetav)$ is called a {\em synthetic likelihood} (SL) model \cite{wood2010statistical}.  For a SL log-likelihood model, we compute estimators of the first and second moments of $\pi(\x|\thetav)$.  For Gaussian $\epsilon$-kernel, we can convolve the two densities
% \begin{eqnarray}
%   \mathcal{L}(\thetavpmd) & = & \log \int \mathcal{N}( \y | \x, \eps^2 ) \mathcal{N}( \x |
%   \mu_{\thetavpm}, \sigma^2_{\thetavpm} ) d\x \\
%                           & = & \log \mathcal{N}( \y | \mu_{\thetavpm}, \sigma^2_{\thetavpm} \eps^2 )
% \end{eqnarray}
We can see that $\epsilon$ now can be set very small as it now acts as a smoother for our estimate, resulting in a robust gradient.
 
\subsection{Gradients in High Dimensions}

%\subsection{Spall's method: SPSA}
In the gradient-free setting, Spall \cite{spall1992multivariate} provides a stochastic approximate to the true gradient using only 2 forward simulations (function evaluations).  This is in contrast to multivariate finite-difference stochastic approximation FDSA \cite{kiefer1952stochastic} requiring $2p$ evaluations.  

The gradient estimate is
\begin{equation}
  \gradestimate{t}{\thetav_t} = \begin{bmatrix} 
                                   \frac{\feval{t}{+} - \feval{t}{-}}{2 \cstep{t}{1} \dstep{t}{1}} \\
                                   \vdots \\
                                   \frac{\feval{t}{+} - \feval{t}{-}}{2 \cstep{t}{p} \dstep{t}{p}} \\
                                \end{bmatrix}
\end{equation}
where $\cstep{t}{p}$ is a step-size that is usually constant for all dimensions $p$, but can be different (as shown in this case); $\dstep{t}{p} \in {-1, +1}$ is a {\em perturbation mask} (called symmetric Bernouilli variables by Spall), i.e. $\dstep{t}{p} \sim 2*\text{Bernouilli}(0.5) - 1$; and  $\feval{t}{\pm}$ are function evaluations:
\begin{eqnarray}
  \feval{t}{+} &= \loglik{\thetav + \cstepconst{t}\dstepconst{t}} \\
  \feval{t}{-} &= \loglik{\thetav - \cstepconst{t}\dstepconst{t}}
\end{eqnarray}
A way of reducing the noise in the gradient is by averaging over $q$ draws of $\dstepconst{t}$:
\begin{equation}
  \gradestimate{t}{\thetav_t} = \frac{1}{q} \sum_{j=1}^q \gradestimaterepeat{t}{\thetav_t}{j}
\end{equation}
where for each $j$ new perturbation masks are drawn.
  

For ABC, the gradients we compute are stochastic because we must approximate $\pi(\x | \thetav)$ with forward simulations.  We can make the approximation arbitrarily good by running more simulations, but this is often unnecessary for sampling, because we can make use of the uncertainty in our current approximation to decide if our sample is correct or not (cite UAI2014?).  

\begin{itemize}
  \item In both cases, we rewrite the expression conditioning on a set of random seeds omega.  Explain the reason for this.  By observing the simulation at different theta but same seeds, we can eliminate much of the noise in the markov chain.  By treating omega as part of the state, we can smoothly move along noiseless outputs for several iterations and then step.
\end{itemize}

\subsection{Stochastic ABC Gradients}
For low-dimensional problems a finite difference (FD) approximation to $\nabla U$.  For each dimension $p$, forward simulate for each random seed twice at plus $c$ and minus $c$ (only along dimension $p$): $x_{\pm}^{(s)} \sim \pi( \x | \thetav \pm c \Delta_p, \omega_s)$.  These simulations are then plugged into the appropriate estimator (logsumsexp or SL).  $\hat{U}_{K_{\eps}}$:
\begin{eqnarray}
  - \log \pi(\thetav \pm c \Delta_p) - \log \sum_s \pi_{\eps}(\y | f( \thetav \pm c \Delta_p, \omega_s ) )
\end{eqnarray}   
or $\hat{U}_{SL}$
\begin{eqnarray}
  - \log \pi(\thetav \pm c \Delta_p) - \log \mathcal{N}(\y | \mu_{\thetav \pm c \Delta_p}, \sigma^2_{\thetav \pm c \Delta_p} + \eps^2 )
\end{eqnarray}


\begin{eqnarray}
  \hat{U}(\thetav)       & = & - \log \pi(\thetav) - \log \pi_{\eps}(\y | \thetav ) \\
  \nabla\hat{U}(\thetav) & = & - \nabla \log \pi(\thetav) - \frac{N}{n} \sum_{i=h_1}^{h_n} \nabla \log \pi(\yi | \thetav ) 
\end{eqnarray} 

\subsection{Common Random Numbers}

[CITE NEAL How to view an MCMC...]
Another variance reduction technique called the method of {\em common random numbers} (CRN) \cite{kleinman1999simulation} sets a common seed for the random number generator (RNG) for both calls to the simulator.  This technique can remove the effect of the simulator noise in the gradient estimate, leaving on the randomness of the perturbation masks as the source of noise. Consider using SPSA instead of SGD: the CRN technique is equivalent of using the same batch of data vectors to evaluate the log-likelihood which is a sensible approach.

Using the same parameter vector $\thetav$ but different random seeds:
\begin{eqnarray}
  \pi\lp \y | \thetav \rp & = &  \int \pi_{\epsvec}\lp \y | \x \rp \pi\lp \x | \thetav \rp d \x \\
   & \approx & \frac{1}{S} \sum_{s=1}^S \pi_{\epsvec}\lp \y | \x^{(s)} \rp
\end{eqnarray}
where $\x^{(s)} = \pi\lp \x | \thetav, \omega_s \rp$.  Note we have passed a unique random seed $\omega_s$ into the simulator, making $\x^{(s)}$ a deterministic function of the simulator with parameters $\{\thetav, \omega_s\}$.  It will be useful to rewrite this deterministic simulator function as $f$: $\x^{(s)} = f\lp \thetav, \omega_s \rp$, allowing us to rewrite the likelihood as
\begin{eqnarray}
  \pi\lp \y | \thetav \rp & \approx & \frac{1}{S} \sum_{s=1}^S \pi_{\epsvec}\lp \y | f\lp \thetav, \omega_s \rp \rp
\end{eqnarray}

% \subsection{Conditional Density Estimators}
% [TODO: TAKEN FROM POPE]  When the simulator is fast and stochastic, it can be beneficial to the inference procedure to build a local, conditional model of the distribution $\pi(\y|\thetav)$ using $S$ simulator responses in $\yone, \ldots, \yS \overset{\simulator}{\sim} \pi( \y | \thetav)$.
% %
% The simplest local response model is the \emph{conditional Gaussian}, an approach called {\em synthetic likelihood} in ABC \cite{wood2010statistical}.   It computes estimators of the first and second moments of the responses and uses the Gaussian distribution to analytically compute the likelihood (thus providing an alternative to kernel ABC). For our algorithms, this allows the direct computation of the CDF:
% \begin{eqnarray}
%  && \muhattheta  =  \frac{1}{S} \sum_{s=1}^{S} \y_s ~~~~~~~~~~~
%  \hat{\Sigma}_{\thetav}  =  \frac{1}{S-1} \sum_{s=1}^{S} \lp \ys - \muhattheta \rp \lp \ys - \muhattheta \rp^T\\
%  &&\F_{\y | \thetav}(\ystar; \muhattheta,  \hat{\Sigma}_{\thetav})  =  \int_{-\infty}^{\ystar} \mathcal{N}\lp \y | \muhattheta,  \hat{\Sigma}_{\thetav} \rp d \y
% \end{eqnarray}
% % \begin{eqnarray}
% %   &&\F_{\y | \thetav}(\ystar; \muhattheta,  \hat{\Sigma}_{\thetav})  =  \int_{-\infty}^{\ystar} \mathcal{N}\lp \y | \muhattheta,  \hat{\Sigma}_{\thetav} \rp d \y
% % \end{eqnarray}
% where $\muhattheta$ and $\hat{\Sigma}_{\thetav}$ are computed from the $S$ simulations.  In our experiments we  often use a factorized model: $\mathcal{N}(\y | \muhattheta,\hat{\Sigma}_{\thetav}) \approx \prod_{j=1}^J \mathcal{N}( y_j | \muhatthetaj,  \sigmahatthetaj)$, resulting in a factorized product over CDFs as well.
% Modeling the response by only the first two moments may be inadequate due to multi-modality, asymmetric noise, etc.   For such cases a \emph{conditional KDE} (kernel density estimate) response model can by used.  In \cite{TurnerGenLik2014} this approach is shown to be superior to conditional Gaussians for certain computational psychology models.  Note that for Gaussian kernels the conditional KDE is very similar to kernel ABC, but has additional flexibility of adaptively choosing bandwidths (rather than the fixed $\epsvec$ in kernel ABC).
% [TODO: END TAKEN FROM POPE]


  

\subsection{Algorithms}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Demonstration}\label{sec:demo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[FROM UAI] In this illustrative problem we infer the rate of an exponential distribution under a gamma prior with shape $\alpha$ and rate $\beta$, having $N$ observations at the true rate $\theta^{\star}$; this is the {\em exponential example} in \cite{turner2012tutorial}.  Let $\w$ be a vector of $N$ draws from an exponential distribution, i.e. $w_n \sim \expdist(\theta)$.  The posterior is a gamma distribution with shape $\alpha+N$ and rate $\beta + \sum w_n$.  To use this problem with ABC, we use the exponential draws as the simulator and the mean of $\w$ as the statistic $y$ and assume that $N$ is known.  The inference problem is therefore to sample from $p( \theta | y, \alpha, \beta, N )$.

\subsection{First order CRNs}
Using same seed in a gradient estimate at time $t$ (versus not using them).  This is makes sense if we compare to SG-data where a single batch is used for computing the gradient.  If a FD method was used, we would use the same batch.

\subsection{Second order CRNs}
Using same seed in a gradient estimate at multiple time steps (versus not changing them -- the previous example).  Analogous to keeping the same batch for consecutive steps in SG-data.

\subsection{Comparing Gradient Estimates with SL}
\begin{itemize}
  \item Can estimate ABC posterior by CLT of Gammas (sum of exponential).  For large S, the variance of the SL approximates the ``true'' CLT approximation.  Thus we can set $\epsilon$ much smaller for the SL case.  The kernel logsumexp suffers when $\epsilon$ is small, using only the closest simulation to $\y$.  Instead the SL estimates a density around the simulations and the likelihood is less sensitive to $\epsilon$.
  \item Better tolerance to low $\epsilon$ using SL.
\end{itemize}

\subsection{Using variance of gradients in SGHMC}
Compute online $\hat{B}$ then fix.

\subsection{Visualizing Noise from Gradient versus Injected Noise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Blowfly}\label{sec:bf}
[FROM UAI] Adult blowfly populations exhibit dynamic behavior for which several competing population models exist.  In this experiment, we use observational data and a simulation model from Wood \cite{wood2010statistical}, based on their improvement upon previous population dynamics theory.  Population dynamics are modeled using (discretized) differential equations that can produce chaotic behavior for some parameter settings.  An example blowfly replicate series is shown in Figure~\ref{fig:blowflyexamples}, along with  times-series generated by a sample from $\p(\thetav|\y)$ using GPS-ABC.
 

In \cite{wood2010statistical} there are several explanations of the population dynamics, corresponding to different simulations and parameters.  We concentrate on the equation (1) in section 1.2.3 of the supplementary information, considered ``a better alternative model" by the author.  The population dynamics equation generates  $N_1, \ldots, N_T$ using the following update rule:
\begin{equation}
N_{t+1} = P N_{t-\tau} \exp(-N_{t-\tau}/N_0) e_t + N_t \exp(-\delta \epsilon_t) \nonumber
\end{equation}
where $e_t \sim  \mathcal{G}( 1/{\sigma_p^2},1/{\sigma_p^2})$ and $\epsilon_t 
 \sim  \mathcal{G}( 1/{\sigma_d^2},1/{\sigma_d^2})$  
are sources of noise, and $\tau$ is an integer (not to be confused with the $\tau$ used as the MH acceptance threshold in our algorithms).  In total, there are 6 parameters $\theta = \{ \log P, \log \delta, \log N_0, \log \sigma_d, \log \sigma_p, \tau\}$.  See \cite{wood2010statistical} for further details about the significance of the parameters.  We put Gaussian priors over all the parameters (with Gaussian proposal distributions), except for $\tau$ which has a Poisson prior (and a left/right increment proposal). Time-series generated with parameters from this prior distribution produce extremely varied results, some are chaotic, some are degenerate, etc.  Modeling this simulator is {\em very} challenging.

As with any ABC problem the choice of statistics is important as it directly affects the quality of the results.   It is also non-trivial and requires careful thought and sometimes trial and error.   In total there are 10 statistics: the log of the mean of all $25\%$ quantiles of $N/1000$ (4 statistics), the mean of the $25\%$ quantiles of the first-order differences of $N/1000$ (4 statistics), and the maximal peaks of smoothed $N$, with 2 different thresholds (2 statistics).    With these statistics it is possible to reproduce time-series that appear similar to the observations.  Note that these statistics are different from Wood's, but they capture similar time-series features and are sufficient to produce credible population dynamics. [END FROM UAI]


\subsection{Bayesian Autoencoders}\label{sec:auto}
We perform Bayesian inference on an autoencoder neural network using MNIST images as observations.  We apply two inference algorithms: SGNHT and HABC.  For SGNHT, we use the known gradient and for HABC we apply 2SPSA.  After 1000 iterations we collect parameter vectors every 100 iterations for a total of 100 sets. (Do we use SGHMC to estimate Bhat in first 1000 iterations?).  Use same batches at each iteration for SGNHT and HABC? 

results: convergence of training error (batches), convergence in test error using parameters collected (record nbr of forward or backward passes required), show mean and variance of filters (how?), compare with using the last parameter sample only (stochastic optimization).





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DISCUSSION} \label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{itemize}
%   \item
%   \item
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSION} \label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
  \item New set of ABC algorithms for efficient exploration of posterior distribution.
  \item Key: opens the door to high-dimensional ABC inference.
\end{itemize}

A further complication, which we do not address on this paper, is the forward simulation might be, and for many interesting problems is, expensive.  We would therefore prefer to resort to some sort of surrogate modeling \cite{Meeds2014GpsUai,wilkinson:2014, michael} to avoid simulations whenever possible. We address this scenario in Section~\ref{future}.  Shown for relatively fast simulators.  Extensions to surrogates with GPS (Meeds2014GpsUai, rasmussen:2003, wilkinson:2014) can use derivative of GP for likelihood as GP of gradient.  These could be matched by occasional gradient estimates (Osborne).

\subsubsection*{References}
{
\bibliographystyle{icml2014}
\bibliography{abcsgld}
}

\end{document}